08/21/2022 21:33:49 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/21/2022 21:33:52 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=400, cache_dir='', config_name='', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_test=True, do_train=False, epoch=5, eval_all_checkpoints=False, eval_batch_size=64, eval_data_file='../data/bug_type/bugtype_buffer_overflow/valid.jsonl', evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='microsoft/codebert-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='./saved_models', output_folder_name='bugtype_buffer_overflow', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=64, per_gpu_train_batch_size=32, save_steps=50, save_total_limit=None, seed=1, server_ip='', server_port='', start_epoch=0, start_step=0, test_data_file='../data/bug_type/bugtype_buffer_overflow/test.jsonl', tokenizer_name='microsoft/codebert-base', train_batch_size=32, train_data_file='../data/bug_type/bugtype_buffer_overflow/train.jsonl', warmup_steps=0, weight_decay=0.0)
08/21/2022 21:34:01 - INFO - __main__ -   ***** Running evaluation *****
08/21/2022 21:34:01 - INFO - __main__ -     Num examples = 3729
08/21/2022 21:34:01 - INFO - __main__ -     Batch size = 64
08/21/2022 21:34:22 - INFO - __main__ -   ***** Eval results *****
08/21/2022 21:34:22 - INFO - __main__ -     eval_acc = 0.9287
08/21/2022 21:34:22 - INFO - __main__ -     eval_loss = 0.2253
08/21/2022 21:34:29 - INFO - __main__ -   ***** Running Test *****
08/21/2022 21:34:29 - INFO - __main__ -     Num examples = 3729
08/21/2022 21:34:29 - INFO - __main__ -     Batch size = 64
  0%|          | 0/59 [00:00<?, ?it/s]  2%|▏         | 1/59 [00:00<00:20,  2.90it/s]  3%|▎         | 2/59 [00:00<00:19,  2.90it/s]  5%|▌         | 3/59 [00:01<00:19,  2.89it/s]  7%|▋         | 4/59 [00:01<00:19,  2.88it/s]  8%|▊         | 5/59 [00:01<00:18,  2.88it/s] 10%|█         | 6/59 [00:02<00:18,  2.88it/s] 12%|█▏        | 7/59 [00:02<00:18,  2.87it/s] 14%|█▎        | 8/59 [00:02<00:17,  2.87it/s] 15%|█▌        | 9/59 [00:03<00:17,  2.87it/s] 17%|█▋        | 10/59 [00:03<00:17,  2.86it/s] 19%|█▊        | 11/59 [00:03<00:16,  2.86it/s] 20%|██        | 12/59 [00:04<00:16,  2.86it/s] 22%|██▏       | 13/59 [00:04<00:16,  2.86it/s] 24%|██▎       | 14/59 [00:04<00:15,  2.86it/s] 25%|██▌       | 15/59 [00:05<00:15,  2.86it/s] 27%|██▋       | 16/59 [00:05<00:15,  2.86it/s] 29%|██▉       | 17/59 [00:05<00:14,  2.86it/s] 31%|███       | 18/59 [00:06<00:14,  2.86it/s] 32%|███▏      | 19/59 [00:06<00:13,  2.86it/s] 34%|███▍      | 20/59 [00:06<00:13,  2.86it/s] 36%|███▌      | 21/59 [00:07<00:13,  2.85it/s] 37%|███▋      | 22/59 [00:07<00:12,  2.85it/s] 39%|███▉      | 23/59 [00:08<00:12,  2.85it/s] 41%|████      | 24/59 [00:08<00:12,  2.85it/s] 42%|████▏     | 25/59 [00:08<00:11,  2.85it/s] 44%|████▍     | 26/59 [00:09<00:11,  2.86it/s] 46%|████▌     | 27/59 [00:09<00:11,  2.86it/s] 47%|████▋     | 28/59 [00:09<00:10,  2.87it/s] 49%|████▉     | 29/59 [00:10<00:10,  2.87it/s] 51%|█████     | 30/59 [00:10<00:10,  2.86it/s] 53%|█████▎    | 31/59 [00:10<00:09,  2.86it/s] 54%|█████▍    | 32/59 [00:11<00:09,  2.86it/s] 56%|█████▌    | 33/59 [00:11<00:09,  2.86it/s] 58%|█████▊    | 34/59 [00:11<00:08,  2.86it/s] 59%|█████▉    | 35/59 [00:12<00:08,  2.86it/s] 61%|██████    | 36/59 [00:12<00:08,  2.87it/s] 63%|██████▎   | 37/59 [00:12<00:07,  2.88it/s] 64%|██████▍   | 38/59 [00:13<00:07,  2.87it/s] 66%|██████▌   | 39/59 [00:13<00:06,  2.88it/s] 68%|██████▊   | 40/59 [00:13<00:06,  2.89it/s] 69%|██████▉   | 41/59 [00:14<00:06,  2.88it/s] 71%|███████   | 42/59 [00:14<00:05,  2.89it/s] 73%|███████▎  | 43/59 [00:14<00:05,  2.89it/s] 75%|███████▍  | 44/59 [00:15<00:05,  2.89it/s] 76%|███████▋  | 45/59 [00:15<00:04,  2.89it/s] 78%|███████▊  | 46/59 [00:16<00:04,  2.89it/s] 80%|███████▉  | 47/59 [00:16<00:04,  2.89it/s] 81%|████████▏ | 48/59 [00:16<00:03,  2.88it/s] 83%|████████▎ | 49/59 [00:17<00:03,  2.88it/s] 85%|████████▍ | 50/59 [00:17<00:03,  2.88it/s] 86%|████████▋ | 51/59 [00:17<00:02,  2.87it/s] 88%|████████▊ | 52/59 [00:18<00:02,  2.87it/s] 90%|████████▉ | 53/59 [00:18<00:02,  2.86it/s] 92%|█████████▏| 54/59 [00:18<00:01,  2.86it/s] 93%|█████████▎| 55/59 [00:19<00:01,  2.85it/s] 95%|█████████▍| 56/59 [00:19<00:01,  2.84it/s] 97%|█████████▋| 57/59 [00:19<00:00,  2.84it/s] 98%|█████████▊| 58/59 [00:20<00:00,  2.84it/s]100%|██████████| 59/59 [00:20<00:00,  2.90it/s]

Accuracy: 0.9337624027889515
Precision: 0.6240601503759399
F-measure: 0.40193704600484265
Recall: 0.29642857142857143
